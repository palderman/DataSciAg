---
title: "Programaming with Data Assignment"
author: |
  | Data Science for Agriculture and Natural Resources
  | PLNT 5413 - Fall `r format(Sys.Date(),'%Y')`
output: pdf_document
link-citations: true
urlcolor: blue
margin: 1in
fontsize: 12pt
geometry: "left=1in,right=1in,top=1in,bottom=1.25in"
header-includes:
- \usepackage{lastpage}
- \usepackage{fancyhdr}
- \newcommand{\aspace}[1]{\vspace{#1\baselineskip}}
- \renewcommand{\headrulewidth}{0pt}
- \fancypagestyle{plain}{ \fancyhf{} \fancyfoot[C]{\aspace{2}Page \thepage~of~\pageref{LastPage}} }
- \pagestyle{fancy}
- \fancyhf{}
- \fancyfoot[C]{\small \thetitle \\ Data Science for Agriculture and Natural Resources\\PLNT 5413 - Fall `r format(Sys.Date(),'%Y')`\\ Page \thepage~of~\pageref{LastPage}}
---

```{r setup,echo=FALSE}
knitr::opts_chunk$set(echo=FALSE,include=TRUE)
```

```{r program_assignment_code,echo=FALSE,include=FALSE}
library(tidyverse)

# Generate separate files for each forest division
tree_raw <- read_tsv('Macroplot_data_Rev.txt')
site_data <- read_tsv('Site_variables.txt')
joined <- site_data %>% select(PlotID,ForestDiv) %>% right_join(tree_raw,by='PlotID')

dir.create('data')
for(fd in unique(joined$ForestDiv)){
  filter(joined,ForestDiv==fd) %>%
    select(-ForestDiv) %>%
    write_tsv(.,paste0('data/',fd,'_tree_data.txt'))
  filter(site_data,ForestDiv==fd) %>%
    select(-ForestDiv) %>%
    write_tsv(.,paste0('data/',fd,'_site_data.txt'))
}
```

```{r answer_key_header,echo=FALSE,include=FALSE}
cat('Total possible points should equal 30\n')
cat('  - Check for loading dependencies (1 pt)\n')
cat('  - Check for comment header for each problem (1 pt)\n')
cat('  - Check for comments explaining workflow (1 pt)\n')
```

In the *Wrangle* assignment, we wrangled a dataset sampled from southern India. In that case, the dataset had already been combined and curated fairly well before we started working with it. When we are working with "real" datasets (particularly multi-location datasets), we often start with multiple files and have to do some serious work importing and combining the data to put it in a form we can use. Using programming skills related to workflows (pipelines), functions, and iteration can greatly facilitate that process. For this assignment, we will focus on that context for practicing proper application of these skills.

Download the attached zip archive file. Create a new subdirectory within your assignment directory called `data` and extract the zip archive into that directory. You'll notice that the data are organized by forest division with one file of tree data and one file of site data for each forest division. Your overall goal for this assignment will be to combine these individual files into a unified dataset prepared for analysis. The general approach we will use is to process the data by forest division and then combine the results into a final dataset. Don't forget to load package dependencies and add comments to the code to mark each problem and explain each step of the workflow.

1. The first stage in this process is to get a list of forest divisions that we can iterate over. Write a function called `list_forest_divisions()` that: (4 pts)
    * Takes a data directory path as an argument
    * Finds files in that directory
    * Strips off any extra parts (e.g. file path, extension, etc.) of the file names
    * Returns a list that includes each forest division name exactly once

        *Hint: The `list.files()` function, the [Strings](http://r4ds.had.co.nz/strings.html) chapter, and the `stringr` package will all be useful for this one.*
        
```{r problem_1_code}
# Problem 1
list_forest_divisions <- function(path){
  # Load required packages
  require(tidyverse)

  for_div_list <- list.files(path = path) %>% # list all files in the path directory
    str_replace_all('(.*/)|(_.*)','') %>% # Strip off any leading directories and trailing file extenstions including _tree_data or _site_data labels
    unique() # Remove duplicate entries in resulting list
  
  return(for_div_list)
}
```

```{r problem_1_output,echo=FALSE,include=TRUE}
for_divs <- list_forest_divisions('data')
for_divs
```

2. Now that we have a list of forest divisions, we need to be able to read and process the data within a given forest division. Write a function called `read_forest_division()` that: (7 pts)
    * Takes a data directory path and a forest division name as arguments
    * Converts the path and forest division name into file path and names for tree and site data files
    * Reads tree data into a tibble and marks '0' tree girth observations as missing (i.e.`NA`)
    * Reads the site data into a tibble while using the `col_types` argument to read only the plot ID, latitude, longitude, altitude, and rain columns
    * Joins the tree and site data tibbles into a single tibble
    * Adds a column named `ForestDiv` to the joined tibble
    * Returns the resulting tibble

        *Hint: Use `?read_tsv` to see an explanation of the arguments and some examples*
        
```{r problem_2_code}
# Problem 2
read_forest_division <- function(directory,for_div){
  # Load required packages
  require(tidyverse)

  tree_data <- paste0(directory,'/',for_div,'_tree_data.txt') %>% # Combine directory path and forest division to create tree data file name
    read_tsv(na = "0",                                  # Read tree data setting "0" observations to NA
             col_types = cols(PlotID = col_character(), # Specifying column types
                              SpCode = col_character(),
                              TreeGirth1 = col_double(),
                              TreeGirth2 = col_double(),
                              TreeGirth3 = col_double(),
                              TreeGirth4 = col_double(),
                              TreeGirth5 = col_double()))
  
  site_data <- paste0(directory,'/',for_div,'_site_data.txt') %>% # Combine directory path and forest division to create site data file name
    read_tsv(col_types = cols_only(PlotID = col_character(),      # Read site data selecting columns and specifying column types
                                   LatDec = col_double(),
                                   LongDec = col_double(),
                                   Alt = col_double(),
                                   Rain = col_double()))

  combined_data <- right_join(site_data,tree_data,by='PlotID') %>% # Join site data and tree data
    mutate(ForestDiv=for_div)                       # Add column for forest division
  
  return(combined_data)   # Return combined data
}
```

```{r problem_2_output,echo=FALSE,include=TRUE}
cat(paste0('Example output for ',for_divs[1],'\n'))
for_div_1 <- read_forest_division('data',for_divs[1])
for_div_1
```
    
3. We now have the pieces we need to build our data input workflow. Write a function called `read_all_divisions()` that: (6 pts)
    * Takes a data directory path as an argument
    * Calls the `list_forest_divisions()` function to generate a list of forest divisions
    * Calls the `read_forest_division()` function for each element of the list generated by the `list_forest_divisions()` function and capturing the output into a single object
    * Converts the object generated by the calls to `read_forest_division()` into a single tibble
    * Gathers the tree girth data into a single column, removes missing observations, and returns the resulting tibble

```{r problem_3_code}
read_all_divisions <- function(path){
  # Load required packages
  require(tidyverse)
  
  all_data <- list_forest_divisions(path) %>% # Generate list of forest divisions
    map(read_forest_division,   # Iterate reading data over each forest division
        directory = path) %>% 
    bind_rows() %>%             # Combine all forest division data into single tibble
    gather('tree','girth',TreeGirth1:TreeGirth5) %>% # Gather tree girth data into a single column
    drop_na()  # Drop missing observations

  return(all_data) # Return 
}

```

```{r problem_3_output,echo=FALSE,include=TRUE}
all_data <- read_all_divisions('data')
all_data
```

4. Based on [Brown (1997)](http://www.fao.org/docrep/W4095E/w4095e06.htm#3.2.1%20biomass%20regression%20equations) there are separate equations that should be used to estimate forest biomass depending on location rainfall and size of tree diameter. Simplifying somewhat, for rainfall < 1600mm use the equation $$Biomass = 0.136Diameter^{2.32}$$ for rainfall $\ge$ 1600 mm and diameter < 160 cm use the equation  $$Biomass = 0.118Diameter^{2.53}$$ and for rainfall $\ge$ 1600 mm and diameter $\ge$ 160 cm $$Biomass = 42.69-12.8Diameter+1.242Diameter^2$$ Write a function called `estimate_biomass()` that: (4 pts)
    * Takes rain and girth as arguments
    * Calculates diameter
    * Calculates biomass with the appropriate equation based on rain and diameter
    * Returns the estimated biomass
            * Hint: Check out both `?if` and `?ifelse`

```{r problem_4_code}
estimate_biomass <- function(rain, girth){
  diameter <- girth/pi
  biomass <- ifelse(rain < 1600,
                    0.136*diameter^2.32,
                    #exp(-1.996+2.32*log(diameter)),
                    ifelse(diameter < 160,
                           0.118*diameter^2.53,
                           #exp(-2.134+2.53*log(diameter)),
                           42.69 - 12.8*diameter + 1.242*diameter^2)
  )
  return(biomass)
}
```

```{r problem_4_output,echo=FALSE,include=TRUE}
rain = c(1100,1800,1800)
girth = round(c(75*pi,125*pi,200*pi),0)
for(i in 1:3){
  cat(paste0('Example output for rain = ',rain[i],' and girth = ',girth[i],':\n'))
  print(estimate_biomass(rain[i],girth[i]))
}
```

5. Use the `estimate_biomass()` function with the tibble generated by the `read_all_divisions()` function to estimate tree biomass and store it in a new column of the original tibble. (2 pts)

```{r problem_5_code}
all_data <- all_data %>%
  mutate(biomass=estimate_biomass(Rain, girth))
```

```{r problem_5_output,echo=FALSE,include=TRUE}
all_data
```

6. For 4 out of the 5 preceding problems you should be able to use the pipe operator at least once to simplify the workflow and make your code more readable. Review your code for these problems and add the pipe operator where it makes sense to do so. (4 pts)

